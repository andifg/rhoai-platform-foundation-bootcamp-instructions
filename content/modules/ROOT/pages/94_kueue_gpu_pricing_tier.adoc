= Lab Guide: Kueue for different GPU pricing tiers
:icons: font
:stem: latexmath
:icons: font
:toc: left
:source-highlighter: highlight.js
:numbered:

== Prerequisites

1.  **OpenShift AI Operator:** Ensure the OpenShift AI Operator is installed on your cluster.
2.  **GPU Worker Node:** You need at least one worker node with a minimum of 1 GPU. On AWS, a `g5.2xlarge` instance or similar is suitable.
3.  **GPU Node Taint:** The GPU node must be tainted to ensure only GPU-tolerant workloads are scheduled on it.

[NOTE]
====
This was already done during the bootstrap process in the previous lab. If you need to reapply the taint, use the following command, replacing `<your-gpu-node-name>` with the actual name of your GPU node.

[.console-input]
[source,bash]
----
oc adm taint nodes <your-gpu-node-name> nvidia.com/gpu=Exists:NoSchedule
----

====

== Use Case description
A customer has an OpenShift cluster and needs to implement Kueue to manage GPU workloads with two distinct pricing models. The first is a *reserved* plan for premium customers who pay for guaranteed, immediate access to their allocated GPU resources. The second is an *on-demand* plan for general users who are charged per use but may have to wait in a queue until GPU capacity becomes available. Kueue will be used to enforce these different service levels, ensuring that reserved workloads are given priority and on-demand jobs are handled fairly based on available resources.
Customers may need *reserved* capacity when their workloads are critical, have strict deadlines, or require predictable performance. Industries such as financial services for high-frequency trading, scientific research with long-running simulations, or media companies for rendering and encoding often need guaranteed access to resources to avoid delays and maintain business continuity. *Reserved* capacity ensures these users always have the compute power they've paid for, eliminating the risk of waiting for resources to become available.
*On-demand* capacity is suitable for customers with intermittent, non-critical, or variable workloads. This model is ideal for tasks like ad-hoc data analysis, development and testing environments, or temporary spikes in demand from a product launch.
Users pay only for what they use, which can be more cost-effective than reserving resources that may sit idle. The trade-off is that they might have to wait for resources during peak usage times, but for these types of workloads, the flexibility and lower cost outweigh the potential for delays.

*Reserved Plan:* A customer who pays for a reserved plan has a guarantee of resources. In this model, you would create a `guaranteed` ResourceFlavor that a user's workloads would request. Kueue would prioritize these workloads, ensuring they get scheduled as long as the reserved capacity is available. üîí

*On-Demand Plan:* This model is for users who pay per use and may need to wait for available resources. You would create an `on-demand ResourceFlavor for these users. Kueue would schedule these jobs only after all higher-priority (reserved) jobs have been scheduled. ‚è≥

.Not just for GPU resources
[NOTE]
====
The focus in this Lab is on the GPU resources but a Kueue does support other resource types as well.
====

== Solution Overview
You'll use *Kueue* to set up a two-tiered system for GPU access. For each *reserved* plan customer who has paid for guaranteed access, you'll create a dedicated `ClusterQueue`. This approach ensures that their workloads are always given a *reserved* GPU, as defined by the `nominalQuota` in their specific `ClusterQueue`. Each customer's `ClusterQueue` will have a `nominalQuota` of one or more GPU, so they are guaranteed to have a resource when they need it.

For *on-demand* users, who do not have a *reserved* plan and can tolerate waiting for resources, you only need a single `ClusterQueue`. This `ClusterQueue` will manage all of the *on-demand* workloads, placing them in a queue and admitting them only when there is available GPU capacity. This single queue handles all requests from multiple on-demand users, providing a cost-effective, pay-as-you-go model.

By separating the *reserved* and *on-demand* workloads into distinct ClusterQueues, you can enforce different service levels. 
As soon as `Cohorts` are supported, Workloads in the reserved ClusterQueues could be prioritized, ensuring that these customers always get their guaranteed resources. *On-demand* jobs will be admitted only when capacity is not being used by the reserved tiers, providing a flexible and efficient way to monetize any leftover resources.

As stated in the official Kueue docs <<kueue-docs>>:

[quote, "Kueue", "Kueue Documentation, Version 0.13.4"]
____
Resources in a cluster are typically not homogeneous. Resources could differ in:

* Pricing and availability (for example, spot versus on-demand VMs)
* Architecture (for example, x86 versus ARM CPUs)
* Brands and models (for example, Radeon 7000 versus Nvidia A100 versus T4 GPUs)

A ResourceFlavor is an object that represents these resource variations and allows you to associate them with cluster nodes through labels, taints and tolerations.
____

In this szenario the label selector is `nvidia.com/gpu.product: NVIDIA-A10G-SHARED`:

[.console-input]
[source,yaml]
----
apiVersion: kueue.x-k8s.io/v1beta1
kind: ResourceFlavor
metadata:
  name: nvidia-a10g-shared
spec:
  nodeLabels:
    nvidia.com/gpu.product: NVIDIA-A10G-SHARED
----

.Not yet supported by the Operator
[NOTE]
====
Usually the customer would like to increase the usage of the GPUs as much as possible. Therefore it would be a good solution to borrow GPU quota between cluster queues.
Every time a GPU within a `ClusterQueue` is unused it can be borrowed by one of the others but has to be released as soon as the original clusterQueue wants to use the resource.

As stated in the official Kueue docs <<kueue-docs>>:

[quote, "Kueue", "Kueue Documentation, Version 0.13.4"]

Cohorts give you the ability to organize your Quotas. ClusterQueues within the same Cohort (or same CohortTree for Hierarchical Cohorts) can share resources with each other. 

[source,yaml]
----
apiVersion: kueue.x-k8s.io/v1beta1
kind: Cohort
metadata:
  name: gpu-sharing-cohort
----
====

This `ClusterQueue` guarantees 4 virtual GPU for customer A.
[.console-input]
[source,yaml]
----
apiVersion: kueue.x-k8s.io/v1beta1
kind: ClusterQueue
metadata:
  name: reserved-capacity-customer-a
spec:
  # cohort: gpu-sharing-cohort
  namespaceSelector: {}
  resourceGroups:
    - coveredResources:
        - "nvidia.com/gpu"
      flavors:
        - name: nvidia-a10g-shared
          resources:
            - name: "nvidia.com/gpu"
              nominalQuota: 4
              # borrowingLimit: 12 # Allows borrowing up to 5 additional GPUs - not supported yet
----

This `ClusterQueue` guarantees 4 virtual GPU for customer B.
[.console-input]
[source,yaml]
----
apiVersion: kueue.x-k8s.io/v1beta1
kind: ClusterQueue
metadata:
  name: reserved-capacity-customer-b
spec:
  # cohort: gpu-sharing-cohort
  namespaceSelector: {}
  resourceGroups:
    - coveredResources:
        - "nvidia.com/gpu"
      flavors:
        - name: nvidia-a10g-shared
          resources:
            - name: "nvidia.com/gpu"
              nominalQuota: 4
              # borrowingLimit: 12 # Allows borrowing up to 5 additional GPUs - not supported yet
----

This `ClusterQueue` gurantees 8 GPUs for all customers using the *on-demand* tier.
[.console-input]
[source,yaml]
----
apiVersion: kueue.x-k8s.io/v1beta1
kind: ClusterQueue
metadata:
  name: on-demand-capacity
spec:
  # cohort: gpu-sharing-cohort
  namespaceSelector: {}
  resourceGroups:
    - coveredResources:
        - "nvidia.com/gpu"
      flavors:
        - name: nvidia-a10g-shared
          resources:
            - name: "nvidia.com/gpu"
              nominalQuota: 8
              # borrowingLimit: 8 # Allows borrowing up to 5 additional GPUs - not supported yet
----

With this configuration, each customer has one or more guaranteed virtual GPUs.

== Verify the Solution
The next step is to veryfy the configuration.

=== Create infrastructure
First create a `namespace` and a `localQueue` pointing to the correct ClusterQueue` for each customer.

[.console-input]
[source,yaml]
----
kind: Namespace
apiVersion: v1
metadata:
  name: reserved-team-a
  labels:
    kubernetes.io/metadata.name: reserved-team-a
    kueue.openshift.io/managed: 'true'
---
apiVersion: kueue.x-k8s.io/v1beta1
kind: LocalQueue
metadata:
  namespace: reserved-team-a
  name: reserved-team-a
spec:
  clusterQueue: reserved-capacity-customer-a
----

[.console-input]
[source,yaml]
----
kind: Namespace
apiVersion: v1
metadata:
  name: reserved-team-b
  labels:
    kubernetes.io/metadata.name: reserved-team-b
    kueue.openshift.io/managed: 'true'
---
apiVersion: kueue.x-k8s.io/v1beta1
kind: LocalQueue
metadata:
  namespace: reserved-team-b
  name: reserved-team-b
spec:
  clusterQueue: reserved-capacity-customer-b
----

[.console-input]
[source,yaml]
----
kind: Namespace
apiVersion: v1
metadata:
  name: on-demand-team-a
  labels:
    kubernetes.io/metadata.name: on-demand-team-a
    kueue.openshift.io/managed: 'true'
---
apiVersion: kueue.x-k8s.io/v1beta1
kind: LocalQueue
metadata:
  namespace: on-demand-team-a
  name: on-demand-team-a
spec:
  clusterQueue: on-demand-capacity
----

.Example Job
[.console-input]
[source,yaml]
----
apiVersion: batch/v1
kind: Job
metadata:
  generateName: reserved-capacity-customer-a-
  namespace: <namespace>
  labels:
    kueue.x-k8s.io/queue-name: <local-queue-name>
spec:
  template:
    spec:
      containers:
      - name: sleeper
        image: registry.access.redhat.com/ubi9/ubi:latest
        command: ["/bin/sleep"]
        args: ["300"] # 5 minutes
        resources:
          limits:
            nvidia.com/gpu: 1
          requests:
            nvidia.com/gpu: 1
      restartPolicy: Never
  backoffLimit: 4
----

*Tasks:*

* üîé Verify that each customer can't exceed the number of assinged GPUs
* ‚ùå Remove the label `kueue.x-k8s.io/queue-name: <local-queue-name>` from the `Job` and test to trick the system. Try to submit Jobs to consume more GPUs then allowed.
* ‚åõÔ∏è Add Memeory allocation to the `ClusterQueue` of Customer A to limit the allowed memory to *1Gi* - verify the configuration.
* ‚ûï Add another customer consuming `on-demand` resources - verify each of the teams consuming `on-demand` can get all of the GPUs (8 GPUs is the maximum configured in the `ClusterQueue`) while the other team is on vacation.

[TIP]
====
Use the dashboard which was created eralier to get insights into the state of different resources. Enable *portfowrading* to access http://localhost:3000/[http://localhost:3000/].
====


*Hint:*
Use the dashboard which was created eralier.

image::94-kueue-viz.png[]

[.console-input]
[source,bash]
----
kubectl -n kueue-system port-forward svc/kueue-kueueviz-backend 8080:8080 &
kubectl -n kueue-system set env deployment kueue-kueueviz-frontend REACT_APP_WEBSOCKET_URL=ws://localhost:8080
kubectl -n kueue-system port-forward svc/kueue-kueueviz-frontend 3000:8080
----

Open http://localhost:3000/[http://localhost:3000/] in the browser.

[bibliography]
== References

* [[[kueue-docs, 1]]] Kueue. _Documentation_. Version May 15, 2025. Available from: https://kueue.sigs.k8s.io/docs/overview/.
* [[[repo, 2]]] AI on OpenShift Contrib Repo. _Kueue Preemption Example_. Available from: https://github.com/opendatahub-io-contrib/ai-on-openshift.