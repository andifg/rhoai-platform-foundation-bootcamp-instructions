= *Lab Guide: Exploring Agentic AI with LlamaStack Notebooks*
:icons: font

This lab explores the Llama Stack on Red Hat OpenShift AI (RHOAI), which is currently in Developer Preview.

This lab will guide you through setting up this environment and exploring the core concepts of building advanced, multi-component agentic systems.

== *1. Prerequisites*

*   You have access to a Red Hat OpenShift AI environment.
*   You have access to link:https://red.ht/maas[Model as a Service (MaaS)] to get an API token for models.
*   The LlamaStack operator is enabled and running in the cluster.

== *2. Let's get started*

Here are some exercises to help you get started with Llama Stack.

=== *2.1. Deploy the OpenShift MCP Server*

The OpenShift Model Context Protocol (MCP) server acts as a bridge, allowing the Llama Stack agent to interact with the OpenShift cluster to answer questions about its state.

1.  First, create a new namespace for the MCP server:
+
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: ocp-mcp
----
+
Save this file as `ocp-mcp-namespace.yaml` and apply it using `oc apply -f ocp-mcp-namespace.yaml`.

2.  Next, create a `ServiceAccount` and the necessary `RoleBinding` and `ClusterRoleBinding` to grant it permissions to read resources from the cluster.
+
[source,yaml]
----
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ocp-mcp
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: ocp-mcp
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: edit
subjects:
- kind: ServiceAccount
  name: ocp-mcp
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-admin-ocp-mcp
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: ocp-mcp
  namespace: ocp-mcp
----
+
Save this file as `ocp-mcp-sa.yaml` and apply it using `oc apply -f ocp-mcp-sa.yaml`.

3.  Now, create the `Deployment` for the MCP server.
+
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: ocp-mcp-server
  name: ocp-mcp-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ocp-mcp-server
  template:
    metadata:
      labels:
        app: ocp-mcp-server
        deployment: ocp-mcp-server
    spec:
      containers:
      - args:
        - --sse-port
        - "8000"
        command:
        - ./kubernetes-mcp-server
        image: quay.io/eformat/kubernetes-mcp-server:latest
        imagePullPolicy: Always
        name: ocp-mcp-server
        ports:
        - containerPort: 8000
          name: http
          protocol: TCP
        resources: {}
      serviceAccountName: ocp-mcp
----
+
Save this file as `ocp-mcp-deployment.yaml` and apply it using `oc apply -f ocp-mcp-deployment.yaml`.

4.  Finally, create the `Service` to expose the MCP server within the cluster.
+
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ocp-mcp-server
  name: ocp-mcp-server
spec:
  ports:
  - port: 8000
    protocol: TCP
    targetPort: http
  selector:
    app: ocp-mcp-server
    deployment: ocp-mcp-server
----
+
Save this file as `ocp-mcp-service.yaml` and apply it using `oc apply -f ocp-mcp-service.yaml`.

=== *2.2. Setting Up Llama Stack Resources*

1.  First, create a new namespace for the Llama Stack components:
+
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: llama-stack
----
+  
Save this file as `namespace.yaml` and apply it using `oc apply -f namespace.yaml`.

2.  Next, create a secret to store your API keys. This file defines three separate secrets: two for the different language models (Llama 3 and Llama 4) and one for the Tavily search tool.
+
[source,yaml]
----
kind: Secret
apiVersion: v1
metadata:
  name: llama-3-2-3b
  namespace: llama-stack
data:
  apiKey: Y2hhbmdfbWU=
type: Opaque

---
kind: Secret
apiVersion: v1
metadata:
  name: llama-4-scout-17b-16e-w4a16
  namespace: llama-stack
data:
  apiKey: Y2hhbmdfbWU=
type: Opaque

---
kind: Secret
apiVersion: v1
metadata:
  name: tavily-search-key
  namespace: llama-stack
data:
  tavily-search-api-key: Y2hhbmdfbWU=
type: Opaque

----
+  
Apply the secret using `oc apply -f creds.yaml`.

3.  Now, create the `ConfigMap` for the Llama Stack. Most of the key configuration for the LlamaStack server is done in the `run.yaml` file, which is encapsulated within this `ConfigMap`. For this lab, we will be focusing on a few of its APIs, such as `inference` and `tool_runtime`. Save the following as `llama-stack-config.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: llama-stack-config
  namespace: llama-stack
data:
  run.yaml: |
    # Llama Stack configuration
    version: '2'
    image_name: vllm
    apis:
    - agents
    - inference
    - safety
    - tool_runtime
    - vector_io
    models:
      - metadata: {}
        model_id: llama-3-2-3b
        provider_id: vllm-llama-3-2-3b
        provider_model_id: llama-3-2-3b
        model_type: llm
      - metadata: {}
        model_id: llama-4-scout-17b-16e-w4a16
        provider_id: vllm-llama-4-guard
        provider_model_id: llama-4-scout-17b-16e-w4a16
        model_type: llm
    providers:
      agents:
      - provider_id: meta-reference
        provider_type: inline::meta-reference
        config:
          persistence_store:
            type: sqlite
            db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/agents_store.db
          responses_store:
            type: sqlite
            db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/responses_store.db
      inference:
      - provider_id: vllm-llama-3-2-3b
        provider_type: "remote::vllm"
        config:
          url: "https://llama-3-2-3b-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443/v1"
          max_tokens: 110000
          api_token: ${env.LLAMA_3_2_3B_API_TOKEN}
          tls_verify: true
      - provider_id: vllm-llama-4-guard
        provider_type: "remote::vllm"
        config:
          url: "https://llama-4-scout-17b-16e-w4a16-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443/v1"
          max_tokens: 110000
          api_token: ${env.LLAMA_4_SCOUT_17B_16E_W4A16_API_TOKEN}
          tls_verify: true
      tool_runtime:
      - provider_id: model-context-protocol
        provider_type: remote::model-context-protocol
        config: {}
      - provider_id: tavily-search
        provider_type: remote::tavily-search
        config:
          api_key: ${env.TAVILY_API_KEY}
          max_results: 3
    tools:
      - name: builtin::websearch
        enabled: true
    tool_groups:
    - provider_id: tavily-search
      toolgroup_id: builtin::websearch
    - toolgroup_id: mcp::openshift
      provider_id: model-context-protocol
      mcp_endpoint:
        uri: http://ocp-mcp-server.ocp-mcp.svc.cluster.local:8000/sse
    server:
      port: 8321
----
+  
Apply the `ConfigMap` using `oc apply -f llama-stack-config.yaml`.

4.  Finally, create the `LlamaStackDistribution`. Save the following as `llama-stack-distro.yaml`:
+
[source,yaml]
----
apiVersion: llamastack.io/v1alpha1
kind: LlamaStackDistribution
metadata:
  name: llamastack-with-config
  namespace: llama-stack
spec:
  replicas: 1
  server:
    containerSpec:
      env:
      - name: TELEMETRY_SINKS
        value: console, sqlite, otel_trace
      - name: OTEL_TRACE_ENDPOINT
        value: http://otel-collector-collector.observability-hub.svc.cluster.local:4318/v1/traces
      - name: OTEL_METRIC_ENDPOINT
        value: http://otel-collector-collector.observability-hub.svc.cluster.local:4318/v1/metrics
      - name: OTEL_SERVICE_NAME
        value: llamastack
      - name: LLAMA_3_2_3B_API_TOKEN
        valueFrom:
          secretKeyRef:
            key: apiKey
            name: llama-3-2-3b
      - name: LLAMA_4_SCOUT_17B_16E_W4A16_API_TOKEN
        valueFrom:
          secretKeyRef:
            key: apiKey
            name: llama-4-scout-17b-16e-w4a16
      - name: TAVILY_API_KEY
        valueFrom:
          secretKeyRef:
            key: tavily-search-api-key
            name: tavily-search-key
      name: llama-stack
      port: 8321
    distribution:
      name: remote-vllm
    userConfig:
      configMapName: llama-stack-config
----
+  
Apply the distribution using `oc apply -f llama-stack-distro.yaml`.

5.  Validate that the Llama Stack server is running correctly. Check the logs of the pod to ensure that it has successfully connected to the models and the OpenShift MCP server.
+
[source,bash]
----
oc logs -n llama-stack $(oc get pods -n llama-stack -l app=llamastack-with-config -o name | head -n 1)
----
+
Look for messages indicating successful connections and that the server is ready to accept requests.

== *3. Interact with Llama Stack using the Python Client*

Now that the Llama Stack server is running, you can interact with it from a Data Science workbench using the `llama-stack` Python client.

1.  In your workbench, open a new notebook.

2.  First, install the `llama-stack` client library:
+
[source,python]
----
!pip install -qq llama-stack
----

3.  Import the necessary libraries:
+
[source,python]
----
import os
from llama_stack_client import LlamaStackClient, Agent, AgentEventLogger
from rich.pretty import pprint
----

4.  Define the connection details for the Llama Stack server. By default, this will use the internal Kubernetes service name.
+
[source,python]
----
LLAMA_STACK_SERVER_HOST = os.getenv("LLAMA_STACK_SERVER_HOST", "llamastack-with-config-service.llama-stack.svc.cluster.local")
LLAMA_STACK_SERVER_PORT = os.getenv("LLAMA_STACK_SERVER_PORT", "8321")
----

5.  Instantiate the client and create an agent. This agent is configured to use the `llama-3-2-3b` model and has access to both the web search and OpenShift tools.
+
[source,python]
----
client = LlamaStackClient(base_url=f"http://{LLAMA_STACK_SERVER_HOST}:{LLAMA_STACK_SERVER_PORT}")

agent = Agent(
    client,
    model="llama-3-2-3b",
    instructions="You are a helpful assistant",
    tools=[
        "builtin::websearch",
        "mcp::openshift"
    ],
    max_infer_iters=5,
    sampling_params={
        "strategy": {"type": "top_p", "temperature": 0.7, "top_p": 0.95},
        "max_tokens": 2048,
    },
)
session_id = agent.create_session("monitored_session")
----

6.  Now you can ask the agent questions. This first example uses the web search tool to find the current OpenShift release.
+
[source,python]
----
response = agent.create_turn(
    messages=[{"role": "user", "content": "Whats the current openshift release?"}],
    session_id=session_id,
)

for log in AgentEventLogger().log(response):
    log.print()
----

7.  This second example uses the OpenShift tool to query for namespaces within the cluster.
+
[source,python]
----
response = agent.create_turn(
    messages=[{"role": "user", "content": "What namespaces are existing inside the cluster?"}],
    session_id=session_id,
)

for log in AgentEventLogger().log(response):
    log.print()
----

== *4. Clean Up*


When you have finished the lab, remember to shut down your workbench from the RHOAI dashboard to release the allocated compute resources.

== *5. References*


* *Llama Stack Demos GitHub Repository*: link:https://github.com/opendatahub-io/llama-stack-demos[Llama Stack Demos]
* *Llama Stack Documentation*: link:https://llama-stack.readthedocs.io/en/latest/[Llama Stack Docs]
* *Model Context Protocol Documentation*: link:https://modelcontextprotocol.io/docs/getting-started/intro[MCP Docs]
