= *Lab Guide: Exploring Agentic AI with LlamaStack Notebooks*
:icons: font

This lab explores the Llama Stack on Red Hat OpenShift AI (RHOAI), which is currently in Developer Preview.

This lab will guide you through setting up this environment and exploring the core concepts of building advanced, multi-component agentic systems.

== *Introduction*

=== *Llama Stack*

Llama Stack is a comprehensive, open-source framework started at Meta, designed to streamline the creation, deployment, and scaling of generative AI applications. It provides a standardized set of tools and APIs that encompass the entire AI development lifecycle, including inference, fine-tuning, evaluation, safety protocols, and the development of agentic systems capable of complex task execution. By offering a unified interface, Llama Stack aims to simplify the often complex process of integrating advanced AI capabilities into various applications and infrastructures. The core purpose of Llama Stack is to empower developers by reducing friction and complexity, allowing them to focus on building innovative and transformative AI solutions. It codifies best practices within the generative AI ecosystem, offering pre-built tools and support for features like tool calling and retrieval augmented generation (RAG). This standardization facilitates a more consistent development experience, whether deploying locally, on-premises, or in the cloud, and fosters greater interoperability within the rapidly evolving generative AI community. Ultimately, Llama Stack seeks to accelerate the adoption and advancement of generative AI by providing a robust and accessible platform for developers of all sizes.

=== *MCP Server*

The open-source Model Context Protocol defines a standard way to connect LLMs to nearly any type of external resources like files, APIs, and databases. It’s built on a client-server system, so applications can easily feed LLMs the context they need.
The OpenShift Model Context Protocol (MCP) Server, which we are going to use in this exercise, lets LLMs interact directly with Kubernetes and OpenShift clusters without needing additional software like kubectl or Helm. It enables operations such as managing pods, viewing logs, installing Helm charts, listing namespaces, etc.—all through a unified interface. This server is lightweight and doesn’t require any external dependencies, making it easy to integrate into existing systems. In the advanced level notebooks, we use this server to connect to the OpenShift cluster, check the status of pods running on the cluster, and report their health and activity.

== *1. Setup & Configuration*

== *1.1 Prerequisites*

*   You have access to a Red Hat OpenShift AI environment.
*   You have access to link:https://red.ht/maas[Model as a Service (MaaS)] to get an API token for LLM models.
*   The LlamaStack operator is enabled and running in the cluster.

=== *1.2 Deploy the OpenShift MCP Server*

The OpenShift Model Context Protocol (MCP) server acts as a bridge, allowing the Llama Stack agent to interact with the OpenShift cluster to answer questions about its state.

1.  First, create a new namespace for the MCP server:
+
[.console-input]
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: ocp-mcp
----
+
Save this file as `ocp-mcp-namespace.yaml` and apply it using `oc apply -f ocp-mcp-namespace.yaml`.

2.  Next, create a `ServiceAccount` and the necessary `RoleBinding` and `ClusterRoleBinding` to grant it permissions to read resources from the cluster.
+
[.console-input]
[source,yaml]
----
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ocp-mcp
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: ocp-mcp
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: edit
subjects:
- kind: ServiceAccount
  name: ocp-mcp
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-admin-ocp-mcp
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: ocp-mcp
  namespace: ocp-mcp
----
+
Save this file as `ocp-mcp-sa.yaml` and apply it using `oc apply -f ocp-mcp-sa.yaml`.

3.  Now, create the `Deployment` for the MCP server.
+
[.console-input]
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: ocp-mcp-server
  name: ocp-mcp-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ocp-mcp-server
  template:
    metadata:
      labels:
        app: ocp-mcp-server
        deployment: ocp-mcp-server
    spec:
      containers:
      - args:
        - --sse-port
        - "8000"
        command:
        - ./kubernetes-mcp-server
        image: quay.io/eformat/kubernetes-mcp-server:latest
        imagePullPolicy: Always
        name: ocp-mcp-server
        ports:
        - containerPort: 8000
          name: http
          protocol: TCP
        resources: {}
      serviceAccountName: ocp-mcp
----
+
Save this file as `ocp-mcp-deployment.yaml` and apply it using `oc apply -f ocp-mcp-deployment.yaml`.

4.  Finally, create the `Service` to expose the MCP server within the cluster.
+
[.console-input]
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ocp-mcp-server
  name: ocp-mcp-server
spec:
  ports:
  - port: 8000
    protocol: TCP
    targetPort: http
  selector:
    app: ocp-mcp-server
    deployment: ocp-mcp-server
----
+
Save this file as `ocp-mcp-service.yaml` and apply it using `oc apply -f ocp-mcp-service.yaml`.

=== *1.3 Setting Up Llama Stack Server Resources*

1. First, create a new namespace for the Llama Stack Server components:

+
[.console-input]
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: llama-stack
----
+
Save this file as `namespace.yaml` and apply it using `oc apply -f namespace.yaml`.

2. Create MAAS API Keys.
+
Go to link:https://red.ht/maas[Model as a Service (MaaS)] to get an API token for the Llama 3 as well as Llama 4 models.
+

3. Create Teavily API Keys.

+
Go to link:https://www.tavily.com/[Teavily] to register and create an API token.
+

4. Next, create a secret to store your API keys. This file defines three separate secrets: two for the different language models (Llama 3 and Llama 4) and one for the Tavily search tool.
+
[.console-input]
[source,yaml]
----
kind: Secret
apiVersion: v1
metadata:
  name: llama-3-2-3b
  namespace: llama-stack
stringData:
  apiKey: <change-me>
type: Opaque
---
kind: Secret
apiVersion: v1
metadata:
  name: llama-4-scout-17b-16e-w4a16
  namespace: llama-stack
stringData:
  apiKey: <change-me>
type: Opaque

---
kind: Secret
apiVersion: v1
metadata:
  name: tavily-search-key
  namespace: llama-stack
stringData:
  apiKey: <change-me>
type: Opaque

----
+  
Apply the secret using `oc apply -f creds.yaml`.

5.  Now, create the `ConfigMap` for the Llama Stack. Most of the key configuration for the LlamaStack server is done in the `run.yaml` file, which is encapsulated within this `ConfigMap`. For this lab, we will be focusing on a few of its APIs, such as `inference` and `tool_runtime`. Save the following as `llama-stack-config.yaml`:
+
[.console-input]
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: llama-stack-config
  namespace: llama-stack
data:
  run.yaml: |
    # Llama Stack configuration
    version: '2'
    image_name: vllm
    apis:
    - agents
    - inference
    - safety
    - tool_runtime
    - vector_io
    models:
      - metadata: {}
        model_id: llama-3-2-3b
        provider_id: vllm-llama-3-2-3b
        provider_model_id: llama-3-2-3b
        model_type: llm
      - metadata: {}
        model_id: llama-4-scout-17b-16e-w4a16
        provider_id: vllm-llama-4-guard
        provider_model_id: llama-4-scout-17b-16e-w4a16
        model_type: llm
    providers:
      agents:
      - provider_id: meta-reference
        provider_type: inline::meta-reference
        config:
          persistence_store:
            type: sqlite
            db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/agents_store.db
          responses_store:
            type: sqlite
            db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/responses_store.db
      inference:
      - provider_id: vllm-llama-3-2-3b
        provider_type: "remote::vllm"
        config:
          url: "https://llama-3-2-3b-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443/v1"
          max_tokens: 110000
          api_token: ${env.LLAMA_3_2_3B_API_TOKEN}
          tls_verify: true
      - provider_id: vllm-llama-4-guard
        provider_type: "remote::vllm"
        config:
          url: "https://llama-4-scout-17b-16e-w4a16-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443/v1"
          max_tokens: 110000
          api_token: ${env.LLAMA_4_SCOUT_17B_16E_W4A16_API_TOKEN}
          tls_verify: true
      tool_runtime:
      - provider_id: model-context-protocol
        provider_type: remote::model-context-protocol
        config: {}
      - provider_id: tavily-search
        provider_type: remote::tavily-search
        config:
          api_key: ${env.TAVILY_API_KEY}
          max_results: 3
    tools:
      - name: builtin::websearch
        enabled: true
    tool_groups:
    - provider_id: tavily-search
      toolgroup_id: builtin::websearch
    - toolgroup_id: mcp::openshift
      provider_id: model-context-protocol
      mcp_endpoint:
        uri: http://ocp-mcp-server.ocp-mcp.svc.cluster.local:8000/sse
    server:
      port: 8321
----
+  
Apply the `ConfigMap` using `oc apply -f llama-stack-config.yaml`.

5.  Finally, create the `LlamaStackDistribution`. Save the following as `llama-stack-distro.yaml`:
+
[.console-input]
[source,yaml]
----
apiVersion: llamastack.io/v1alpha1
kind: LlamaStackDistribution
metadata:
  name: llamastack-with-config
  namespace: llama-stack
spec:
  replicas: 1
  server:
    containerSpec:
      env:
      - name: TELEMETRY_SINKS
        value: console, sqlite, otel_trace
      - name: OTEL_TRACE_ENDPOINT
        value: http://otel-collector-collector.observability-hub.svc.cluster.local:4318/v1/traces
      - name: OTEL_METRIC_ENDPOINT
        value: http://otel-collector-collector.observability-hub.svc.cluster.local:4318/v1/metrics
      - name: OTEL_SERVICE_NAME
        value: llamastack
      - name: LLAMA_3_2_3B_API_TOKEN
        valueFrom:
          secretKeyRef:
            key: apiKey
            name: llama-3-2-3b
      - name: LLAMA_4_SCOUT_17B_16E_W4A16_API_TOKEN
        valueFrom:
          secretKeyRef:
            key: apiKey
            name: llama-4-scout-17b-16e-w4a16
      - name: TAVILY_API_KEY
        valueFrom:
          secretKeyRef:
            key: tavily-search-api-key
            name: tavily-search-key
      name: llama-stack
      port: 8321
    distribution:
      name: rh-dev
    storage:
      mountPath: /opt/app-root/src/
      size: 10Gi
    userConfig:
      configMapName: llama-stack-config
----
+  
Apply the distribution using `oc apply -f llama-stack-distro.yaml`.

6.  Validate that the Llama Stack server is running correctly. Check the logs of the pod to ensure that it has successfully connected to the models and the OpenShift MCP server.
+
[.console-input]
[source,bash]
----
oc logs -n llama-stack $(oc get pods -n llama-stack -l app=llama-stack -o name | head -n 1)
----
+
Look for messages indicating successful connections and that the server is ready to accept requests.

== *2. Agentic AI Agents with Llama Stack Clients*

In the next steps we are going to create multiple Agents using the llama stack client python package. All agents will be defined within the same Jupyter Notebook.

=== *2.1 Notebook Setup*

1.  Within a workbench of your choice, open a new notebook.

2.  First, install the `llama-stack` client library:
+
[.console-input]
[source,python]
----
!pip install -qq llama-stack
----

3.  Import the necessary libraries:
+
[.console-input]
[source,python]
----
import os
from llama_stack_client import LlamaStackClient, Agent, AgentEventLogger
from rich.pretty import pprint
----
+

4.  Define the connection details for the Llama Stack server. By default, this will use the internal Kubernetes service name.
+
[.console-input]
[source,python]
----
LLAMA_STACK_SERVER_HOST = os.getenv("LLAMA_STACK_SERVER_HOST", "llamastack-with-config-service.llama-stack.svc.cluster.local")
LLAMA_STACK_SERVER_PORT = os.getenv("LLAMA_STACK_SERVER_PORT", "8321")
----

=== *2.2 Agent 1 - Web Search*

Instantiate the client and create an agent. This agent is configured to use the `llama-4-scout-17b-16e-w4a16` model and has access to both the web search.
[.console-input]
[source,python]
----
client_webserach = LlamaStackClient(base_url=f"http://{LLAMA_STACK_SERVER_HOST}:{LLAMA_STACK_SERVER_PORT}")

agent_websearch = Agent(
    client_webserach,
    model="llama-4-scout-17b-16e-w4a16",
    instructions="You are a helpful assistant",
    tools=[
        "builtin::websearch",
    ],
    max_infer_iters=5,
    sampling_params={
        "strategy": {"type": "top_p", "temperature": 0.1, "top_p": 0.95},
    },
)
session_websearch = agent_websearch.create_session("monitored_session")
----

Now you can ask the agent questions. This first example uses the web search tool to find the current OpenShift release.
[.console-input]
[source,python]
----
response = agent_websearch.create_turn(
    messages=[{"role": "user", "content": "Whats the current openshift release?"}],
    session_id=session_websearch,
)

for log in AgentEventLogger().log(response):
    log.print()
----

=== *2.3 Agent 2 - Openshift MCP Server*

The second agent we create has access to the openshift MCP Server to retrieve openshift api information
[.console-input]
[source,python]
----
client_mcp = LlamaStackClient(base_url=f"http://{LLAMA_STACK_SERVER_HOST}:{LLAMA_STACK_SERVER_PORT}")

agent_mcp = Agent(
    client_mcp,
    model="llama-3-2-3b",
    instructions="You are a helpful assistant",
    tools=[
        "mcp::openshift"
    ],
    max_infer_iters=5,
    sampling_params={
        "strategy": {"type": "top_p", "temperature": 0.1, "top_p": 0.95},
        "max_tokens": 8000,
    },
)
session_mcp = agent_mcp.create_session("monitored_session")
----

Its now possible to ask the agent questions about the openshift cluster and its able to receive data via the mcp server:
[.console-input]
[source,python]
----
response = agent_mcp.create_turn(
    messages=[{"role": "user", "content": "What namespaces are existing inside the cluster?"}],
    session_id=session_mcp,
)

for log in AgentEventLogger().log(response):
    log.print()
----

=== *2.4 Agent 3 - Websearch & MCP*

The third agent we create has access to the mcp server as well as the web search.
[.console-input]
[source,python]
----
client_mutli = LlamaStackClient(base_url=f"http://{LLAMA_STACK_SERVER_HOST}:{LLAMA_STACK_SERVER_PORT}")

agent_multi = Agent(
    client_mutli,
    model="llama-4-scout-17b-16e-w4a16",
    instructions="You are an assistant helping to debug openshift cluster issues. Always backup your findings by using the websearch tool",
    tools=[
        "mcp::openshift",
        "builtin::websearch"
    ],
    max_infer_iters=5,
    sampling_params={
        "strategy": {"type": "top_p", "temperature": 0.1, "top_p": 0.95},
        "max_tokens": 8000,
    },
)
session_multi = agent_multi.create_session("monitored_session")
----

Its now possible to ask the agent questions about the openshift cluster and its able to receive data via the mcp server:
[.console-input]
[source,python]
----
response = agent_multi.create_turn(
    messages=[{"role": "user", "content": "Search for pods having problems in the default namespace using the openshift mcp. If you find one, search for fixes using the websearch"}],
    session_id=session_multi,
)

for log in AgentEventLogger().log(response):
    log.print()
----



== *3. References*


* *Llama Stack Demos GitHub Repository*: link:https://github.com/opendatahub-io/llama-stack-demos[Llama Stack Demos]
* *Llama Stack Documentation*: link:https://llama-stack.readthedocs.io/en/latest/[Llama Stack Docs]
* *Model Context Protocol Documentation*: link:https://modelcontextprotocol.io/docs/getting-started/intro[MCP Docs]
