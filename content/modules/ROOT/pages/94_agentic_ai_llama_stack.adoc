= *Lab Guide: Exploring Agentic AI with LlamaStack Notebooks*
:icons: font

This lab explores the Llama Stack on Red Hat OpenShift AI (RHOAI), which is currently in Developer Preview.

This lab will guide you through setting up this environment and exploring the core concepts of building advanced, multi-component agentic systems.

== *Introduction*

=== *Llama Stack*

Llama Stack is a comprehensive, open-source framework started at Meta, designed to streamline the creation, deployment, and scaling of generative AI applications. It provides a standardized set of tools and APIs that encompass the entire AI development lifecycle, including inference, fine-tuning, evaluation, safety protocols, and the development of agentic systems capable of complex task execution. By offering a unified interface, Llama Stack aims to simplify the often complex process of integrating advanced AI capabilities into various applications and infrastructures. The core purpose of Llama Stack is to empower developers by reducing friction and complexity, allowing them to focus on building innovative and transformative AI solutions. It codifies best practices within the generative AI ecosystem, offering pre-built tools and support for features like tool calling and retrieval augmented generation (RAG). This standardization facilitates a more consistent development experience, whether deploying locally, on-premises, or in the cloud, and fosters greater interoperability within the rapidly evolving generative AI community. Ultimately, Llama Stack seeks to accelerate the adoption and advancement of generative AI by providing a robust and accessible platform for developers of all sizes.

=== *MCP Server*

The open-source Model Context Protocol defines a standard way to connect LLMs to nearly any type of external resources like files, APIs, and databases. It’s built on a client-server system, so applications can easily feed LLMs the context they need.
The OpenShift Model Context Protocol (MCP) Server, which we are going to use in this exercise, lets LLMs interact directly with Kubernetes and OpenShift clusters without needing additional software like kubectl or Helm. It enables operations such as managing pods, viewing logs, installing Helm charts, listing namespaces, etc.—all through a unified interface. This server is lightweight and doesn’t require any external dependencies, making it easy to integrate into existing systems. In the advanced level notebooks, we use this server to connect to the OpenShift cluster, check the status of pods running on the cluster, and report their health and activity.

== *1. Setup & Configuration*

== *1.1 Prerequisites*

*   You have access to a Red Hat OpenShift AI environment.
*   You have access to link:https://red.ht/maas[Model as a Service (MaaS)] to get an API token for LLM models.
*   The LlamaStack operator is enabled and running in the cluster.

=== *1.2 Deploy the OpenShift MCP Server*

The OpenShift Model Context Protocol (MCP) server acts as a bridge, allowing the Llama Stack agent to interact with the OpenShift cluster to answer questions about its state.

1.  First, create a new namespace for the MCP server:
+
[.console-input]
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: ocp-mcp
----
+
Save this file as `ocp-mcp-namespace.yaml` and apply it using `oc apply -f ocp-mcp-namespace.yaml`.

2.  Next, create a `ServiceAccount` and the necessary `RoleBinding` and `ClusterRoleBinding` to grant it permissions to read resources from the cluster.
+
[.console-input]
[source,yaml]
----
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ocp-mcp
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: ocp-mcp
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: edit
subjects:
- kind: ServiceAccount
  name: ocp-mcp
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-admin-ocp-mcp
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: ocp-mcp
  namespace: ocp-mcp
----
+
Save this file as `ocp-mcp-sa.yaml` and apply it using `oc apply -f ocp-mcp-sa.yaml`.

3.  Now, create the `Deployment` for the MCP server.
+
[.console-input]
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: ocp-mcp-server
  name: ocp-mcp-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ocp-mcp-server
  template:
    metadata:
      labels:
        app: ocp-mcp-server
        deployment: ocp-mcp-server
    spec:
      containers:
      - args:
        - --sse-port
        - "8000"
        command:
        - ./kubernetes-mcp-server
        image: quay.io/eformat/kubernetes-mcp-server:latest
        imagePullPolicy: Always
        name: ocp-mcp-server
        ports:
        - containerPort: 8000
          name: http
          protocol: TCP
        resources: {}
      serviceAccountName: ocp-mcp
----
+
Save this file as `ocp-mcp-deployment.yaml` and apply it using `oc apply -f ocp-mcp-deployment.yaml`.

4.  Finally, create the `Service` to expose the MCP server within the cluster.
+
[.console-input]
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ocp-mcp-server
  name: ocp-mcp-server
spec:
  ports:
  - port: 8000
    protocol: TCP
    targetPort: http
  selector:
    app: ocp-mcp-server
    deployment: ocp-mcp-server
----
+
Save this file as `ocp-mcp-service.yaml` and apply it using `oc apply -f ocp-mcp-service.yaml`.

=== *1.3 Setting Up Llama Stack Server Resources*

1. First, create a new namespace for the Llama Stack Server components:

+
[.console-input]
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: llama-stack
----
+
Save this file as `namespace.yaml` and apply it using `oc apply -f namespace.yaml`.

2. Create MAAS API Keys.
+
Go to link:https://red.ht/maas[Model as a Service (MaaS)] and login using your Red Hat credentials to get an API token for the Llama 3 as well as Llama 4 models.
+

3. Create Teavily API Keys.

+
Go to link:https://www.tavily.com/[Teavily] to register and create an API token.
+

4. Next, create a secret to store your API keys. This file defines three separate secrets: two for the different language models (Llama 3 and Llama 4) and one for the Tavily search tool.
+
[.console-input]
[source,yaml]
----
kind: Secret
apiVersion: v1
metadata:
  name: llama-3-2-3b
  namespace: llama-stack
stringData:
  apiKey: <change-me>
type: Opaque
---
kind: Secret
apiVersion: v1
metadata:
  name: llama-4-scout-17b-16e-w4a16
  namespace: llama-stack
stringData:
  apiKey: <change-me>
type: Opaque

---
kind: Secret
apiVersion: v1
metadata:
  name: tavily-search-key
  namespace: llama-stack
stringData:
  apiKey: <change-me>
type: Opaque

----
+  
Apply the secret using `oc apply -f creds.yaml`.

5.  Now, create the `ConfigMap` for the Llama Stack. Most of the key configuration for the LlamaStack server is done in the `run.yaml` file, which is encapsulated within this `ConfigMap`. For this lab, we will be focusing on a few of its APIs, such as `inference` and `tool_runtime`. Save the following as `llama-stack-config.yaml`:
+
[.console-input]
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: llama-stack-config
  namespace: llama-stack
data:
  run.yaml: |
    # Llama Stack configuration
    version: '2'
    image_name: vllm
    apis:
    - agents
    - inference
    - safety
    - tool_runtime
    - vector_io
    - files
    models:
      - metadata: {}
        model_id: llama-3-2-3b
        provider_id: vllm-llama-3-2-3b
        provider_model_id: llama-3-2-3b
        model_type: llm
      - metadata: {}
        model_id: llama-4-scout-17b-16e-w4a16
        provider_id: vllm-llama-4-guard
        provider_model_id: llama-4-scout-17b-16e-w4a16
        model_type: llm
      - metadata:
          embedding_dimension: 768
        model_id: ibm-granite/granite-embedding-125m-english
        provider_id: sentence-transformers
        model_type: embedding
    providers:
      files:
      - provider_id: localfs
        provider_type: inline::localfs
        config:
          storage_dir: /opt/app-root/src/.llama/files
          metadata_store:
            type: sqlite
            db_path: /opt/app-root/src/.llama/files_metadata.db
      vector_io:
      - provider_id: milvus
        provider_type: inline::milvus
        config:
          db_path: /opt/app-root/src/.llama/milvus.db
          kvstore:
            type: sqlite
            db_path: /opt/app-root/src/.llama/milvus_registry.db
      agents:
      - provider_id: meta-reference
        provider_type: inline::meta-reference
        config:
          persistence_store:
            type: sqlite
            db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/agents_store.db
          responses_store:
            type: sqlite
            db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/responses_store.db
      inference:
      - provider_id: sentence-transformers
        provider_type: inline::sentence-transformers
        config: {}
      - provider_id: vllm-llama-3-2-3b
        provider_type: "remote::vllm"
        config:
          url: "https://llama-3-2-3b-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443/v1"
          max_tokens: 110000
          api_token: ${env.LLAMA_3_2_3B_API_TOKEN}
          tls_verify: true
      - provider_id: vllm-llama-4-guard
        provider_type: "remote::vllm"
        config:
          url: "https://llama-4-scout-17b-16e-w4a16-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443/v1"
          max_tokens: 110000
          api_token: ${env.LLAMA_4_SCOUT_17B_16E_W4A16_API_TOKEN}
          tls_verify: true
      tool_runtime:
      - config: {}
        provider_id: rag-runtime
        provider_type: inline::rag-runtime
      - provider_id: model-context-protocol
        provider_type: remote::model-context-protocol
        config: {}
      - provider_id: tavily-search
        provider_type: remote::tavily-search
        config:
          api_key: ${env.TAVILY_API_KEY}
          max_results: 3
    tools:
      - name: builtin::websearch
        enabled: true
    tool_groups:
    - toolgroup_id: builtin::rag
      provider_id: rag-runtime
      args:
        vector_db_ids: ["default-vector-db"]
    - provider_id: tavily-search
      toolgroup_id: builtin::websearch
    - toolgroup_id: mcp::openshift
      provider_id: model-context-protocol
      mcp_endpoint:
        uri: http://ocp-mcp-server.ocp-mcp.svc.cluster.local:8000/sse
    shields: []
    vector_dbs:
      - vector_db_id: default-vector-db
        provider_id: milvus
        embedding_model: ibm-granite/granite-embedding-125m-english
        embedding_dimension: 768
    datasets: []
    scoring_fns: []
    benchmarks: []
    server:
      port: 8321
----
+  
Apply the `ConfigMap` using `oc apply -f llama-stack-config.yaml`.

5.  Finally, create the `LlamaStackDistribution`. Save the following as `llama-stack-distro.yaml`:
+
[.console-input]
[source,yaml]
----
apiVersion: llamastack.io/v1alpha1
kind: LlamaStackDistribution
metadata:
  name: llamastack-with-config
  namespace: llama-stack
spec:
  replicas: 1
  server:
    containerSpec:
      env:
      - name: TELEMETRY_SINKS
        value: console, sqlite, otel_trace
      - name: OTEL_TRACE_ENDPOINT
        value: http://otel-collector-collector.observability-hub.svc.cluster.local:4318/v1/traces
      - name: OTEL_METRIC_ENDPOINT
        value: http://otel-collector-collector.observability-hub.svc.cluster.local:4318/v1/metrics
      - name: OTEL_SERVICE_NAME
        value: llamastack
      - name: LLAMA_3_2_3B_API_TOKEN
        valueFrom:
          secretKeyRef:
            key: apiKey
            name: llama-3-2-3b
      - name: LLAMA_4_SCOUT_17B_16E_W4A16_API_TOKEN
        valueFrom:
          secretKeyRef:
            key: apiKey
            name: llama-4-scout-17b-16e-w4a16
      - name: TAVILY_API_KEY
        valueFrom:
          secretKeyRef:
            key: tavily-search-api-key
            name: tavily-search-key
      name: llama-stack
      port: 8321
    distribution:
      name: rh-dev
    storage:
      mountPath: /opt/app-root/src/
      size: 10Gi
    userConfig:
      configMapName: llama-stack-config
----
+  
Apply the distribution using `oc apply -f llama-stack-distro.yaml`.

6.  Validate that the Llama Stack server is running correctly. Check the logs of the pod to ensure that it has successfully connected to the models and the OpenShift MCP server.
+
[.console-input]
[source,bash]
----
oc logs -n llama-stack $(oc get pods -n llama-stack -l app=llama-stack -o name | head -n 1)
----
+
Look for messages indicating successful connections and that the server is ready to accept requests.

== *2. Agentic AI Agents with Llama Stack Clients*

In the next steps we are going to create multiple Agents using the llama stack client python package. All agents will be defined within the same Jupyter Notebook.

=== *2.1 Notebook Setup*

1.  Within a workbench of your choice, open a new notebook.

2.  First, install the `llama-stack` client library:
+
[.console-input]
[source,python]
----
!pip install -qq llama-stack
----

3.  Import the necessary libraries:
+
[.console-input]
[source,python]
----
import os
from llama_stack_client import LlamaStackClient, Agent, AgentEventLogger
from rich.pretty import pprint
----
+

4.  Define the connection details for the Llama Stack server. By default, this will use the internal Kubernetes service name.
+
[.console-input]
[source,python]
----
LLAMA_STACK_SERVER_HOST = os.getenv("LLAMA_STACK_SERVER_HOST", "llamastack-with-config-service.llama-stack.svc.cluster.local")
LLAMA_STACK_SERVER_PORT = os.getenv("LLAMA_STACK_SERVER_PORT", "8321")
----

=== *2.2 Agent 1 - Web Search*

Instantiate the client and create an agent. This agent is configured to use the `llama-4-scout-17b-16e-w4a16` model and has access to both the web search.
[.console-input]
[source,python]
----
client_webserach = LlamaStackClient(base_url=f"http://{LLAMA_STACK_SERVER_HOST}:{LLAMA_STACK_SERVER_PORT}")

agent_websearch = Agent(
    client_webserach,
    model="llama-4-scout-17b-16e-w4a16",
    instructions="You are a helpful assistant",
    tools=[
        "builtin::websearch",
    ],
    max_infer_iters=5,
    sampling_params={
        "strategy": {"type": "top_p", "temperature": 0.1, "top_p": 0.95},
    },
)
session_websearch = agent_websearch.create_session("monitored_session")
----

Now you can ask the agent questions. This first example uses the web search tool to find the current OpenShift release.
[.console-input]
[source,python]
----
response = agent_websearch.create_turn(
    messages=[{"role": "user", "content": "Whats the current openshift release?"}],
    session_id=session_websearch,
)

for log in AgentEventLogger().log(response):
    log.print()
----

=== *2.3 Agent 2 - Openshift MCP Server*

The second agent we create has access to the openshift MCP Server to retrieve openshift api information
[.console-input]
[source,python]
----
client_mcp = LlamaStackClient(base_url=f"http://{LLAMA_STACK_SERVER_HOST}:{LLAMA_STACK_SERVER_PORT}")

agent_mcp = Agent(
    client_mcp,
    model="llama-3-2-3b",
    instructions="You are a helpful assistant",
    tools=[
        "mcp::openshift"
    ],
    max_infer_iters=5,
    sampling_params={
        "strategy": {"type": "top_p", "temperature": 0.1, "top_p": 0.95},
        "max_tokens": 8000,
    },
)
session_mcp = agent_mcp.create_session("monitored_session")
----

Its now possible to ask the agent questions about the openshift cluster and its able to receive data via the mcp server:
[.console-input]
[source,python]
----
response = agent_mcp.create_turn(
    messages=[{"role": "user", "content": "What namespaces are existing inside the cluster?"}],
    session_id=session_mcp,
)

for log in AgentEventLogger().log(response):
    log.print()
----

=== *2.4 Agent 3 - Websearch & MCP*

The third agent we create has access to the mcp server as well as the web search.
[.console-input]
[source,python]
----
client_mutli = LlamaStackClient(base_url=f"http://{LLAMA_STACK_SERVER_HOST}:{LLAMA_STACK_SERVER_PORT}")

agent_multi = Agent(
    client_mutli,
    model="llama-4-scout-17b-16e-w4a16",
    instructions="You are an assistant helping to debug openshift cluster issues",
    tools=[
        "mcp::openshift",
        "builtin::websearch"
    ],
    max_infer_iters=5,
    sampling_params={
        "strategy": {"type": "top_p", "temperature": 0.1, "top_p": 0.95},
        "max_tokens": 8000,
    },
)
session_multi = agent_multi.create_session("monitored_session")
----

Lets apply a broken pod and let the agent investigate it:
[.console-input]
[source,python]
----
apiVersion: v1
kind: Pod
metadata:
  name: fail-crash-loop
  namespace: default
spec:
  containers:
    - name: alpine
      image: alpine:3.19
      command: ["sh", "-c", "exit 1"] # Always exits with failure
----


Its now possible to ask the agent questions about the openshift cluster and its able to receive data via the mcp server:
[.console-input]
[source,python]
----
messages=[{"role": "user", "content": "Search for pods having problems in the default namespace using the openshift mcp. If you find one, search for fixes using the websearch"},
          {"role": "user", "content": "Which pods are not running?"},
          {"role": "user", "content": "What can i do to stop the pod from failing?"}
         ]
for message in messages: 
    response = agent_multi.create_turn(
      messages=[message],
        session_id=session_multi,
    )
    
    for log in AgentEventLogger().log(response):
        log.print()
----


=== *2.5 Llama Stack Playground*

[.console-input]
[source,python]
----
apiVersion: v1
kind: Namespace
metadata:
  name: llama-stack-playground
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: llama-stack-playground
  labels:
    helm.sh/chart: llama-stack-playground-1.0.0
    app.kubernetes.io/name: llama-stack-playground
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
  annotations:
    serviceaccounts.openshift.io/oauth-redirectreference.primary: '{"kind":"OAuthRedirectReference","apiVersion":"v1","reference":{"kind":"Route","name":"llama-stack-playground"}}'
---
apiVersion: authorization.openshift.io/v1
kind: ClusterRoleBinding
metadata:
  name: system:openshift:scc:anyuid
roleRef:
  name: system:openshift:scc:anyuid
subjects:
- kind: ServiceAccount
  name: llama-stack-playground
  namespace: llama-stack-playground
userNames:
- system:serviceaccount:llama-stack-playground:llama-stack-playground
---
apiVersion: v1
kind: Service
metadata:
  name: llama-stack-playground
  labels:
    helm.sh/chart: llama-stack-playground-1.0.0
    app.kubernetes.io/name: llama-stack-playground
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
  annotations:
    service.alpha.openshift.io/serving-cert-secret-name: proxy-tls
spec:
  type: ClusterIP
  ports:
    - name: proxy
      port: 443
      targetPort: 8443
  selector:
    app.kubernetes.io/name: llama-stack-playground
    app.kubernetes.io/instance: release-name
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-stack-playground
  labels:
    helm.sh/chart: llama-stack-playground-1.0.0
    app.kubernetes.io/name: llama-stack-playground
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: llama-stack-playground
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      labels:
        app.kubernetes.io/name: llama-stack-playground
        app.kubernetes.io/instance: release-name
    spec:
      serviceAccountName: llama-stack-playground
      securityContext:
        runAsNonRoot: true
        runAsUser: 1001
      containers:
        - name: oauth-proxy
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1001
          image: registry.redhat.io/openshift4/ose-oauth-proxy-rhel9@sha256:f55b6d17e2351b32406f72d0e877748b34456b18fcd8419f19ae1687d0dce294
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8443
              name: public
          args:
            - --https-address=:8443
            - --provider=openshift
            - --openshift-service-account=llama-stack-playground
            - --upstream=http://localhost:8501
            - --tls-cert=/etc/tls/private/tls.crt
            - --tls-key=/etc/tls/private/tls.key
            - --cookie-secret=SECRET
          volumeMounts:
            - mountPath: /etc/tls/private
              name: proxy-tls
        - name: llama-stack-playground
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1001
          image: "quay.io/rh_ee_anixel/llama-stack-playground:0.0.4"
          command:
            - streamlit
            - run
            - /llama_stack/core/ui/app.py
          imagePullPolicy: Always
          ports:
          - containerPort: 8501
            name: http
            protocol: TCP
          env:
            - name: STREAMLIT_BROWSER_GATHER_USAGE_STATS
              value: "false"
            - name: STREAMLIT_SERVER_ADDRESS
              value: "0.0.0.0"
            - name: STREAMLIT_SERVER_PORT
              value: "8501"
            - name: LLAMA_STACK_ENDPOINT
              value: "http://llamastack-with-config-service.llama-stack:8321"
            - name: DEFAULT_MODEL
              value: "granite-31-2b-instruct"
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: http
            initialDelaySeconds: 5
            periodSeconds: 5
            timeoutSeconds: 3
          resources:
            limits:
              memory: 1Gi
            requests:
              cpu: 500m
              memory: 512Mi
      volumes:
      - name: proxy-tls
        secret:
          secretName: proxy-tls
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: llama-stack-playground
  labels:
    helm.sh/chart: llama-stack-playground-1.0.0
    app.kubernetes.io/name: llama-stack-playground
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "latest"
    app.kubernetes.io/managed-by: Helm
spec:
  to:
    kind: Service
    name: llama-stack-playground
    weight: 100
  port:
    targetPort: proxy
  tls:
    termination: reencrypt
    insecureEdgeTerminationPolicy: Redirect
----


== *3. References*


* *Llama Stack Demos GitHub Repository*: link:https://github.com/opendatahub-io/llama-stack-demos[Llama Stack Demos]
* *Llama Stack Documentation*: link:https://llama-stack.readthedocs.io/en/latest/[Llama Stack Docs]
* *Model Context Protocol Documentation*: link:https://modelcontextprotocol.io/docs/getting-started/intro[MCP Docs]
